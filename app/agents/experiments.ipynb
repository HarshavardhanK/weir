{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    prompt: str\n",
    "    output: str\n",
    "    \n",
    "class StateWithMessages(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    \n",
    "class StateUserAI(TypedDict):\n",
    "    user_message: List[str]\n",
    "    ai_message: List[str]\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\", \n",
    "                 api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 max_tokens=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt(state: State) -> State:\n",
    "    prompt = state.get('prompt', '')\n",
    "    # Process the prompt (e.g., convert to uppercase)\n",
    "    response = prompt.upper()\n",
    "    # Update the state with the response\n",
    "    state['output'] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "def process_prompt_llm(state: StateWithMessages) -> StateWithMessages:\n",
    "    system_prompt = SystemMessage(content=\"You are an assistant to break down a user's query into sub questions. Examine the query and break it down into relevant and appropriate sub questions that can be answered.\")\n",
    "    messages = state.get('messages', [])\n",
    "    \n",
    "    print('State:', state)\n",
    "    user_input = messages[-1].content if messages else ''\n",
    "    \n",
    "    messages.append(system_prompt)\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    \n",
    "    state['messages'] = messages\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prompt_user_llm(state: StateUserAI) -> StateUserAI:\n",
    "    system_prompt = SystemMessage(content=\"You are an assistant to break down a user's query into sub questions. Examine the query and break it down into relevant and appropriate sub questions that can be answered.\")\n",
    "    user_messages = state.get('user_message', [])\n",
    "    ai_messages = state.get('ai_message', [])\n",
    "    \n",
    "    print('State:', state)\n",
    "    user_input = user_messages[-1].content if user_messages else ''\n",
    "    messages = []\n",
    "    messages.append(system_prompt)\n",
    "    \n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "    print('Messages:', messages)\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    messages.append(AIMessage(content=response.content))\n",
    "    \n",
    "    state['user_message'] = user_messages\n",
    "    state['ai_message'] = messages[-1]\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO, LANGGRAPH!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def naive_graph():\n",
    "\n",
    "    # Initialize the graph with the State structure\n",
    "    graph_builder = StateGraph(State)\n",
    "\n",
    "    # Add the node to the graph\n",
    "    graph_builder.add_node('process_prompt', process_prompt)\n",
    "\n",
    "    # Define the execution flow\n",
    "    graph_builder.add_edge(START, 'process_prompt')\n",
    "    graph_builder.add_edge('process_prompt', END)\n",
    "\n",
    "    # Compile the graph\n",
    "    graph = graph_builder.compile()\n",
    "\n",
    "    # Initial state with a prompt\n",
    "    initial_state = {'prompt': 'Hello, LangGraph!', 'output': ''}\n",
    "\n",
    "    # Execute the graph\n",
    "    final_state = graph.invoke(initial_state)\n",
    "\n",
    "    # Retrieve the output\n",
    "    print(final_state['output'])  # Outputs: 'HELLO, LANGGRAPH!'\n",
    "    \n",
    "naive_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_graph():\n",
    "    graph_builder = StateGraph(StateUserAI)\n",
    "    #graph_builder.add_node('process_prompt', process_prompt_llm)\n",
    "    graph_builder.add_node('process_prompt', process_prompt_user_llm)\n",
    "    #Define the execution flow\n",
    "    graph_builder.add_edge(START, 'process_prompt') #Should be the node name from previous step\n",
    "    graph_builder.add_edge('process_prompt', END)\n",
    "    \n",
    "    graph = graph_builder.compile()\n",
    "    \n",
    "    #initial_state = {'messages': [HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options')]}\n",
    "    initial_state = {'user_message': [HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options')]}\n",
    "    \n",
    "    print('Initial State:', initial_state)\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    print(final_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: {'user_message': [HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options', additional_kwargs={}, response_metadata={})]}\n",
      "State: {'user_message': [HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options', additional_kwargs={}, response_metadata={})]}\n",
      "Messages: [SystemMessage(content=\"You are an assistant to break down a user's query into sub questions. Examine the query and break it down into relevant and appropriate sub questions that can be answered.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options', additional_kwargs={}, response_metadata={})]\n",
      "{'user_message': [HumanMessage(content='I need to fly from New York to San Francisco on the 20th of December. I want to know the cheapest options', additional_kwargs={}, response_metadata={})], 'ai_message': AIMessage(content='To help you find the cheapest options for your flight from New York to San Francisco on the 20th of December, here are some relevant sub-questions:\\n\\n1. What are the major airports in New York and San Francisco that you can fly from and to?\\n2. Are you flexible with your travel dates, or is the 20th of December fixed?\\n3. What is your preferred time of day to fly (morning, afternoon, evening)?\\n4. Are you open to layovers, or do you prefer a direct flight?\\n5. What airlines operate flights between New York and San Francisco?\\n6. Have you checked any specific flight comparison websites or apps for the lowest fares?\\n7. Are you considering any additional costs, such as baggage fees or seat selection?\\n8. Do you have any rewards points or miles that you could use for this flight?\\n9. Are you interested in any specific class of service (economy, business, etc.)? \\n10. Would you like to know about any travel restrictions or requirements for your trip?', additional_kwargs={}, response_metadata={})}\n"
     ]
    }
   ],
   "source": [
    "llm_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sutlej",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
